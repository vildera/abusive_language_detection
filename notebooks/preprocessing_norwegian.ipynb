{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Norwegian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info about the dataset\n",
    "\n",
    "#### Resett\n",
    "We decided to use a subset of 6000 of the collected comments from Resett from Fall 2019. All comments annotated as non-neutral earlier in the project thesis were kept, as well as a sample of 2000 neutral. This decision was due to the imbalance of topics since most of the posts were written against Muslims. Thus, the complete dataset for annotation consisted of 41 910 utterances. We present the amount of data removed in each step of preprocessing in Table 6.1.\n",
    "**NB: Note that the data examination below shows that this is wrong. There are 4K neutral and 2K non-neutral kept**\n",
    "\n",
    "#### Facebook\n",
    "* Removed comments larger than 500 characters\n",
    "* Removed comments shorter than 10 characters\n",
    "* Exchanged Names with \"Navn\"\n",
    "* Removed non-Norwegian instances and URLS\n",
    "* Dropped duplicates\n",
    "* Removed comments only consisting of names\n",
    "\n",
    "\n",
    "#### Twitter\n",
    "* Removed non-Norwegian instances\n",
    "* Removed URLS, via-mentions \n",
    "* Dropped duplicates \n",
    "* Exhanced usernames with @USER \n",
    "* Removed # at the beginning of sentences \n",
    "* Removed tweets directly rom newspaper accounts (DN+ etc.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas_ods_reader import read_ods\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "import re, itertools\n",
    "from string import punctuation\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "import emoji\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+ \"/SvanesEA20/\"\n",
    "\n",
    "fb = pd.read_csv(path + \"facebook_labelled.csv\")\n",
    "tw = pd.read_csv(path + \"tweets_labelled.csv\")\n",
    "rs = pd.read_csv(path + \"resett_labelled.csv\")\n",
    "\n",
    "names = [\"Facebook\", \"Twitter\", \"Resett\"]\n",
    "dfs = [fb, tw, rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    print(names[i] + \": \")\n",
    "    display(dfs[i].head(2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum and minimum lengths of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------\")\n",
    "print(\"Maximum lengths: \")\n",
    "print(\"------------\")\n",
    "for i in range(len(dfs)):\n",
    "    print()\n",
    "    print(names[i] + \": \", dfs[i].text.map(lambda x: len(x)).max())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"Minimum lengths: \")\n",
    "print(\"------------\")\n",
    "for i in range(len(dfs)):\n",
    "    print()\n",
    "    print(names[i] + \": \", dfs[i].text.map(lambda x: len(x)).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfs)):\n",
    "    df = dfs[i]\n",
    "    \n",
    "    print(names[i] + \": \")\n",
    "    #display(dfs[i].category.value_counts())\n",
    "    \n",
    "    df = df.loc[df['category'].isin([str(x) for x in range(1,6)])]\n",
    "    display(df.category.value_counts())\n",
    "    \n",
    "    print(\"Length dataframe:\", len(df))\n",
    "    print()\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = sum([len(df) for df in dfs])\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11366 + 23784 + 5995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print(df.text.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [fb, tw, rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df[\"length\"] = df.text.map(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs: \n",
    "    df = df.loc[df['category'].isin([str(x) for x in range(1,6)])]\n",
    "    print(len(df[df.length < 10]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, remove irrelevant punctuation\n",
    "def clean_text(string):\n",
    "    return ''.join(c for c in string.lower()if c not in punctuation)\n",
    "\n",
    "# Reduce orthographic lengthening to two characters\n",
    "def remove_duplicates(string):\n",
    "    cleaned = ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    return cleaned\n",
    "\n",
    "def sent_tokenize(string):\n",
    "    return nltk.sent_tokenize(string, language = \"norwegian\")\n",
    "\n",
    "def word_tokenize(string):\n",
    "    return nltk.word_tokenize(string, language = \"norwegian\")\n",
    "\n",
    "def convert_user(string):\n",
    "    return re.sub(r'(@User|@USER)', 'Navn', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fb.text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = df.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length[length > 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df[\"cleaned_text\"] = df.text.apply(clean_text).apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert @USER to Navn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no[\"text\"] = no[\"text\"].apply(convert_user)\n",
    "no[\"cleaned\"] = no[\"cleaned\"].apply(convert_user)\n",
    "\n",
    "no[no.source == \"tw\"].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path + 'preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
